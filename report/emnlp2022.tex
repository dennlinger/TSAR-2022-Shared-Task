% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OWN STUFF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{cleveref}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\n}{$\backslash$n}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{The Bitter Lesson Strikes Again:\\ Is Compute All We Need for Lexical Simplification?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Dennis Aumiller \and Michael Gertz \\
  Institute of Computer Science \\
  Heidelberg University \\
  \texttt{\{aumiller, gertz\}@informatik.uni-heidelberg.de}}

\begin{document}
\maketitle
\begin{abstract}
Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential.
In this technical report, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances.
Our best-performing submission to the English language track of the 2022 TSAR shared task consists of an ``ensemble'' of six different prompt templates with varying context levels.
Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of generated suggestions and implications for future work.
\end{abstract}

\section{Introduction}
Across the general landscape of AI research, Richard Sutton coined the idea of a ``bitter lesson'', wherein more computational power will ultimately supersede a hand-crafted solution~\todo{cite bitter lesson}.
While we have previously seen this primarily for Computer Vision research~\todo{cite ImageNet and GoogleNet or something}, more recently, increasing compute has also shown to be wildly successful in the NLP community~\todo{cite emergent behavior paper}.
In particular, emergent capabilities in extremely large language models (xLLMs) have made it possible to approach a variety of tasks wherein only few (if any) samples are provided with labels, and no further fine-tuning is required at all.\\
In stark contrast to the complex setup required for modern lexical simplification systems~\todo{cite context work}, in this work we present a simplistic pipeline based only on prompting a xLLM for lexical simplification, which returns frustratingly good results.
After submitting to the TSAR 2022 shared task for English, standings indicate that this approach may be another instance of the bitter lesson, producing results strictly better than alternative solutions across all evaluated metrics.\\
While the initial findings are indeed promising, we want to cautiously evaluate erroneous instances on the test set to analyze potential pitfalls, and further detail some of our experiences in crafting prompts for context-dependent prediction tasks.
Furthermore, we acknowledge the technical challenges in reproducing (and productionizing) systems based on xLLMs, especially given that suitable models exceed traditional computing budgets.

\todo{somewhere I need to actually mention more details on the TSAR shared task and task data!! This could be enough content for a Background/Rel Work section?}

\section{Prompt-based Lexical Simplification}
\todo{Write about what the general idea is for this model.}
Touted as the ~\todo{insert flashy phrase, such as ``human-level AI''}, the GPT-3 model released by OpenAI~\todo{cite} has been the first in a series of available xLLMs for general-purpose text generation~\todo{cite other works, like BLOOM, OPT, LamBda}. Across these models, a general trend in scaling beyond a particular parameter size can be observed, while keeping the underlying architectural design close to existing smaller models~\todo{maybe cite something}.
After training on enough data for long enough periods, these models exhibit zero-shot transfer capabilities across a wide range of tasks; more precisely, models are able to answer questions formulated in natural language with (more or less) sensible results. Particular template patterns (so-called \emph{prompts}) are frequently used to guide models towards predicting a particularly desirable output or answer format.

\noindent Using this paradigm, we experimented with different prompts issued to OpenAI's largest available model, \texttt{text-davinici-002}, which totals 176B parameters and is available through the OpenAI API\footnote{\todo{URL link}}.
For the final prompts used in our submission, please refer to \Cref{sec:prompts}. We also detail any further hyperparameters and filtering steps used in the pipeline.
Furthermore, we submitted two different runs for the TSAR 2022 shared task, for which we will detail the differences below.

\subsection{Run 1: Zero-shot Prediction}
\todo{Here, we explain the basic setup of the prompting approach, and detail basic hyperparameters (ten responses per prompt that we ask the system to report).}
\todo{Also explain why we use the particular zero-shot prompting approach. Primarily also say that we are limited in the compute budget and the evaluation strategies on trial data with few-shot approaches. We estimate that this serves as a reasonable ``lower-bound'' submission. This is especially the case if we only consider pure zero-context approaches for some of the models}

\subsection{Filtering Predictions}
\todo{Write about how we need to perform basic post-filtering because the output is not always the same. Give maybe an example in a figure?}
The full list of filtering operations is detailed in \Cref{sec:filters}.

\subsection{Run 2: Ensemble Predictions}
Upon inspecting the results from the first run, we noticed that in some instances, predictions were almost fully discarded due to filtering.
Simultaneously, we had already previously encountered strong variance in system generations when varying the prompt template or altering the context setting.
To this extent, an ensemble of predictions from multiple different prompt templates was utilized to broaden the spectrum of possible generations, as well as ensuring that a minimum number of suggestions survives filtering in each instance.

\subsubsection{Prompt Variants}

The added prompts are further detailed in \Cref{tab:prompts}, but can be generally grouped into \emph{with context} (the context sentence is provided), or \emph{without context} (only the complex word is given and synonyms should be generated from there).
Simultaneously, different prompts also contain between zero and two examples with expected outputs for instances from the trial data, to demonstrate the system what correct answers may look like.
Aside from this, we vary the generation temperature, where a higher temperature increases the likelihood of a potentially more creative (but not always correct) prediction.

\subsubsection{Combining Predictions}

For each of the six prompts $p \in P$, we ask the model to generate ten individual suggestions and filter them with the exact same rules as the system in Run 1. In order to combine and re-rank suggestions $S$, we assign the following score to each distinct prediction $s \in S$:
\todo{rewrite this shit to make more sense}
\begin{align}
	\text{score}(s) = \sum_{p \in P} 5.5 - 0.5 \cdot \text{rank}_p(\text{s}),
\end{align}

\todo{Extending this, we can obtain some of the predictions by combining the ranks across the different ensemble models}



\begin{table*}
\centering
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular}
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\
\hline
\end{tabular}
\caption{Results on the English language test set of the TSAR shared task. Listed are our own results (\emph{UniHD}), the two best-performing competing systems~\todo{include}, as well as provided baselines~\todo{name them and cite the relevant paper}.}
\label{tab:accents}
\end{table*}


\section{Error Analysis and Limitations}

As with other sequence-to-sequence tasks, the output of a xLLM cannot be guaranteed to be entirely correct at all times.
In this section, we detail some of the particular challenges we have encountered during the design process.

\subsection{Computational Budgeting}
Running a xLLM in practice, even for inference-only settings, is non-trivial and requires compute that is far beyond many public institution's hardware budget. For the largest models with publicly available checkpoints\footnote{At the time of writing, this would be the 176B Bloom model~\todo{cite}, which has a similar parameter count to OpenAI's davinci-002 model.}, a total of around 320GB \textbf{GPU memory} is required.\\
The common alternative is to obtain predictions through a (generally paid) API, as was the case in this work. Especially for the ensemble model, which issues six individual requests to the API per sample, this can further bloat the net cost of a single prediction.
To give context of the total cost, we incurred a total charge of \$7.15 for computing predictions across the entire test set of 373 English samples for the shared task, which comes to about 1000 tokens per sample, or around \$0.02 at the current OpenAI pricing scheme.\footnote{\url{https://openai.com/api/pricing/}, last accessed: 2022-10-01}


\section{Conclusion and Future Work}
\todo{Maybe ask Matthew/organizers if we can actually get the predictions for the baselines? This way, we could compare the approaches. The context paper also mentions a few instances, so maybe look there?}


%%%%%%%%%%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%%%%%%%%%
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Prompt Templates}
\label{sec:prompts}

\Cref{tab:prompts} provides the exact prompt templates used in the submission. Notably, the \emph{zero-shot with context} prompt is included twice, but with different generation temperatures~\todo{mention which ones, or link to the hyperparameter table?}.

\begin{table*}
	\sloppy
	\hspace*{-2em}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Prompt Type} & \textbf{Template} \\
		\hline
		Zero-shot with context & \texttt{Context: \{context\_sentence\}\n} \\
							   & \texttt{Question: Given the above context, list ten alternatives for} \\
							   & \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
							   &\texttt{Answer:} \\
							   
		\hline
		\hline
		Single-shot with context & \texttt{Context: A local witness said a separate group of attackers disguised}\\
								& \texttt{    in burqas — the head-to-toe robes worn by conservative Afghan women —}\\
								& \texttt{    then tried to storm the compound.\n}\\
								& \texttt{Question: Given the above context, list ten alternative words for}\\
								& \texttt{    ``disguised'' that are easier to understand.\n}\\
								& \texttt{Answer:\n1. concealed\n2. dressed\n3. hidden\n4. camouflaged\n}\\
								& \texttt{    5. changed\n6. covered\n7. masked\n8. unrecognizable\n9. converted\n}\\
								& \texttt{    10. impersonated\n\n}\\
								& \texttt{Context: \{context\_sentence\}\n} \\
								& \texttt{Question: Given the above context, list ten alternatives for} \\
								& \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
								&\texttt{Answer:} \\
		\hline
		Two-shot with context  & \texttt{Context: That prompted the military to deploy its largest warship, }\\
		& \texttt{the BRP Gregorio del Pilar, which was recently acquired from the }\\
		& \texttt{United States.\n}\\
		& \texttt{Question: Given the above context, list ten alternative words for }\\
		& \texttt{``deploy'' that are easier to understand.\n}\\
		& \texttt{Answer:\n1. send\n2. post\n3. use\n4. position\n5. send out\n}\\
		& \texttt{6. employ\n7. extend\n8. launch\n9. let loose\n10. organize\n\n}\\
		& \texttt{Context: The daily death toll in Syria has declined as the number of }\\
		& \texttt{observers has risen, but few experts expect the U.N. plan to succeed }\\
		& \texttt{in its entirety.\n}\\
		& \texttt{Question: Given the above context, list ten alternative words for }\\
		& \texttt{``observers'' that are easier to understand.\n}\\
		& \texttt{Answer:\n1. watchers\n2. spectators\n3. audience\n4. viewers\n}\\
		& \texttt{5. witnesses\n6. patrons\n7. followers\n8. detectives\n9. reporters\n}\\
		& \texttt{10. onlookers\n\n}\\
		& \texttt{Context: \{context\_sentence\}\n} \\
		& \texttt{Question: Given the above context, list ten alternatives for} \\
		& \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
		&\texttt{Answer:} \\
		\hline
		Zero-shot w/o context & \texttt{Give me ten simplified synonyms for the following word: \{complex\_word\}} \\
		\hline
		Single-shot w/o context & \texttt{Question: Find ten easier words for ``compulsory''.\n} \\
								    & \texttt{Answer:\n1. mandatory\n2. required\n3. essential\n4. forced\n}\\
								    & \texttt{    5. important\n6. necessary\n7. obligatory\n8. unavoidable\n} \\
								    & \texttt{    9. binding\n10. prescribed\n\n} \\
								    & \texttt{Question: Find ten easier words for ``\{complex\_word\}''.\n}\\
								    & \texttt{Answer:}\\
		\hline
		
	\end{tabular}
	\caption{The exact prompt templates used for querying the OpenAI model. Only written out \texttt{\n} indicate newlines, visible newlines are inserted for better legibility. The top-most prompt template was used in both runs. The remaining prompts were only included in the ensemble submission (Run 2).}
	\label{tab:prompts}
\end{table*}

\section{Hyperparameters}
We use the OpenAI Python package\footnote{\todo{Insert package URL}}, version 0.23.0 for our experiments. For generation, the function \texttt{openai.Completion.create()} is used, where most hyperparameters remain fixed across all prompts. We explicitly list those hyperparameters below that differ from the default values.
\todo{Pretty sure that most of these also constitute the default parameters? Only include those that activelsy differ!!}
\begin{enumerate}
	\item \texttt{model="text-davinci-002"}, the latest and biggest model for text generation at the time of writing,
	\item \texttt{max\_tokens=256}, to limit generation length,
	\item \texttt{top\_p=1.0}, to include the entire vocabulary during token generation,
	\item \texttt{best\_of=1}, \todo{I think default?},
	\item \texttt{frequency\_penalty=0.5}, \todo{read up on this parameter again? How is it different from the other one?},
	\item \texttt{presence\_penalty=0.3}, which penalizes already present tokens. We choose a lower value, since individual subword tokens might indeed be present several times across multiple (valid) predictions~\todo{maybe explain with an example?}.
\end{enumerate}

\todo{Include table with the associated temperatures.}


\section{Post-Filtering Operations}
\label{sec:filters}
\todo{Include the list of operations by which we filter.}

\end{document}
