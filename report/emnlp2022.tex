% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OWN STUFF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{cleveref}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\n}{$\backslash$n}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{The Bitter Lesson Strikes Again:\\ Is Compute All We Need for Lexical Simplification?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Dennis Aumiller \and Michael Gertz \\
  Institute of Computer Science \\
  Heidelberg University \\
  \texttt{\{aumiller, gertz\}@informatik.uni-heidelberg.de}}

\begin{document}
\maketitle
\begin{abstract}
Previous state-of-the-art for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge to work properly.
In this technical report, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances.
Our best-performing submission to the English language track of the 2022 TSAR shared task consists of an ``ensemble'' of six different prompt templates with varying context levels.
Aside from detailing the implementation and hyperparameters, we spend the remainder of this work discussing the particularities of suggestions generated by our approach and implications for future work.
\end{abstract}

\section{Introduction}
\todo{Cite the Bitter Lesson, the paper about emergent behavior in xLLM, and the hardware lottery wrt setup complexity/cost.}
\todo{Also mention details about the shared task and cite their work, detailing the current SotA a bit more. I'm pretty sure that it doesn't make sense to have a separate section for Related work on four pages, especially since I'm not super familiar with it.}

\section{Prompt-based Lexical Simplification}
\todo{Write about what the general idea is for this model.}

For the exact prompts used in our submission, please refer to \Cref{sec:prompts}. We also detail any further hyperparameters and filtering steps used in the pipeline.

\subsection{Run 1: Zero-shot Prediction}
\todo{Here, we explain the basic setup of the prompting approach, and detail basic hyperparameters (ten responses per prompt that we ask the system to report).}
\todo{Also explain why we use the particular zero-shot prompting approach. Primarily also say that we are limited in the compute budget and the evaluation strategies on trial data with few-shot approaches. We estimate that this serves as a reasonable ``lower-bound'' submission. This is especially the case if we only consider pure zero-context approaches for some of the models}

\subsection{Filtering Predictions}
\todo{Write about how we need to perform basic post-filtering because the output is not always the same. Give maybe an example in a figure?}
The full list of filtering operations is detailed in \Cref{sec:filters}.

\subsection{Run 2: Ensemble Predictions}
\todo{Write how we noticed that for inspections with some of the trial samples we noticed some predictions looked a bit inconsistent (or were empty). This was mostly due to the multi-word expression synonyms, which are not really what annotators provided in the survey answers}

\subsubsection{Utilized Prompt Setups}

\todo{Basically list the six different prompt settings, and what we hope to get from this.}

\subsubsection{Combining Predictions}
\todo{Extending this, we can obtain some of the predictions by combining the ranks across the different ensemble models}



\begin{table*}
\centering
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular}
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\
\hline
\end{tabular}
\caption{Results on the English language test set of the TSAR shared task. Listed are our own results (\emph{UniHD}), the two best-performing competing systems~\todo{include}, as well as provided baselines~\todo{name them and cite the relevant paper}.}
\label{tab:accents}
\end{table*}


\section{Error Analysis and Limitations}

As with other sequence-to-sequence tasks, the output of a xLLM cannot be guaranteed to be entirely correct at all times.
In this section, we detail some of the particular challenges we have encountered during the design process.

\subsection{Computational Budgeting}
Running a xLLM in practice, even for inference-only settings, is non-trivial and requires compute that is far beyond many public institution's hardware budget. For the largest models with publicly available checkpoints\footnote{At the time of writing, this would be the 176B Bloom model~\todo{cite}, which has a similar parameter count to OpenAI's davinci-002 model.}, a total of around 320GB \textbf{GPU memory} is required.\\
The common alternative is to obtain predictions through a (generally paid) API, as was the case in this work. Especially for the ensemble model, which issues six individual requests to the API per sample, this can further bloat the net cost of a single prediction.
To give context of the total cost, we incurred a total charge of \$7.15 for computing predictions across the entire test set of 373 English samples for the shared task, which comes to about 1000 tokens per sample, or around \$0.02 at the current OpenAI pricing scheme.\footnote{\url{https://openai.com/api/pricing/}, last accessed: 2022-10-01}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Prompt Templates}
\label{sec:prompts}

\Cref{tab:prompts} provides the exact prompt templates used in the submission. Notably, the \emph{zero-shot with context} prompt is included twice, but with different generation temperatures~\todo{mention which ones, or link to the hyperparameter table?}.

\begin{table*}
	\sloppy
	\hspace*{-2em}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Prompt Type} & \textbf{Template} \\
		\hline
		Zero-shot with context & \texttt{Context: \{context\_sentence\}\n} \\
							   & \texttt{Question: Given the above context, list ten alternatives for} \\
							   & \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
							   &\texttt{Answer:} \\
							   
		\hline
		\hline
		Single-shot with context & \texttt{Context: A local witness said a separate group of attackers disguised}\\
								& \texttt{    in burqas — the head-to-toe robes worn by conservative Afghan women —}\\
								& \texttt{    then tried to storm the compound.\n}\\
								& \texttt{Question: Given the above context, list ten alternative words for}\\
								& \texttt{    ``disguised'' that are easier to understand.\n}\\
								& \texttt{Answer:\n1. concealed\n2. dressed\n3. hidden\n4. camouflaged\n}\\
								& \texttt{    5. changed\n6. covered\n7. masked\n8. unrecognizable\n9. converted\n}\\
								& \texttt{    10. impersonated\n\n}\\
								& \texttt{Context: \{context\_sentence\}\n} \\
								& \texttt{Question: Given the above context, list ten alternatives for} \\
								& \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
								&\texttt{Answer:} \\
		\hline
		Two-shot with context  & \texttt{Context: That prompted the military to deploy its largest warship, }\\
		& \texttt{the BRP Gregorio del Pilar, which was recently acquired from the }\\
		& \texttt{United States.\n}\\
		& \texttt{Question: Given the above context, list ten alternative words for }\\
		& \texttt{``deploy'' that are easier to understand.\n}\\
		& \texttt{Answer:\n1. send\n2. post\n3. use\n4. position\n5. send out\n}\\
		& \texttt{6. employ\n7. extend\n8. launch\n9. let loose\n10. organize\n\n}\\
		& \texttt{Context: The daily death toll in Syria has declined as the number of }\\
		& \texttt{observers has risen, but few experts expect the U.N. plan to succeed }\\
		& \texttt{in its entirety.\n}\\
		& \texttt{Question: Given the above context, list ten alternative words for }\\
		& \texttt{``observers'' that are easier to understand.\n}\\
		& \texttt{Answer:\n1. watchers\n2. spectators\n3. audience\n4. viewers\n}\\
		& \texttt{5. witnesses\n6. patrons\n7. followers\n8. detectives\n9. reporters\n}\\
		& \texttt{10. onlookers\n\n}\\
		& \texttt{Context: \{context\_sentence\}\n} \\
		& \texttt{Question: Given the above context, list ten alternatives for} \\
		& \texttt{    ``\{complex\_word\}'' that are easier to understand.\n} \\
		&\texttt{Answer:} \\
		\hline
		Zero-shot w/o context & \texttt{Give me ten simplified synonyms for the following word: \{complex\_word\}} \\
		\hline
		Single-shot w/o context & \texttt{Question: Find ten easier words for ``compulsory''.\n} \\
								    & \texttt{Answer:\n1. mandatory\n2. required\n3. essential\n4. forced\n}\\
								    & \texttt{    5. important\n6. necessary\n7. obligatory\n8. unavoidable\n} \\
								    & \texttt{    9. binding\n10. prescribed\n\n} \\
								    & \texttt{Question: Find ten easier words for ``\{complex\_word\}''.\n}\\
								    & \texttt{Answer:}\\
		\hline
		
	\end{tabular}
	\caption{The exact prompt templates used for querying the model. Only \texttt{$\backslash$n} indicate newlines, visible newlines are only inserted for better legibility. The top-most prompt template was used for Run 1, as well as part of the ensemble in Run 2. The remaining prompts were only included in the ensemble.}
	\label{tab:prompts}
\end{table*}

\section{Hyperparameters}
We use the OpenAI Python package\footnote{\todo{Insert package URL}}, version 0.23.0 for our experiments. For generation, the function \texttt{openai.Completion.create()} is used, where most hyperparameters remain fixed across all prompts. We explicitly list those hyperparameters below that differ from the default values.
\todo{Pretty sure that most of these also constitute the default parameters? Only include those that activelsy differ!!}
\begin{enumerate}
	\item \texttt{model="text-davinci-002"}, the latest and biggest model for text generation at the time of writing,
	\item \texttt{max\_tokens=256}, to limit generation length,
	\item \texttt{top\_p=1.0}, to include the entire vocabulary during token generation,
	\item \texttt{best\_of=1}, \todo{I think default?},
	\item \texttt{frequency\_penalty=0.5}, \todo{read up on this parameter again? How is it different from the other one?},
	\item \texttt{presence\_penalty=0.3}, which penalizes already present tokens. We choose a lower value, since individual subword tokens might indeed be present several times across multiple (valid) predictions~\todo{maybe explain with an example?}.
\end{enumerate}

\todo{Include table with the associated temperatures.}


\section{Post-Filtering Operations}
\label{sec:filters}
\todo{Include the list of operations by which we filter.}

\end{document}
